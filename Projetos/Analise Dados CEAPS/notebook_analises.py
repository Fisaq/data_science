# -*- coding: utf-8 -*-
"""Análise de Dados de Prestação de Contas do Senado Brasileiro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EJ3bnV8oyJow8QIXBJK3vbslb1OPY2oL

# **Projeto Ciência de Dados - Desafio 7 Days of Code [Alura]**

O presente notebook consiste na resolução de um desafio proposto pela Alura, onde estarei analisando os dados abertos da CEAPS(Cota para Exercício da Atividade Parlamentar dos Senadores).

O objetivo deste projeto é analisar os dados abertos do Senado, afim  de identificar como os senadores estão utilizando o dinheiro público, bem como propor propostas de melhoria, caso necessário.
"""

# Bibliotecas utilizadas
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import chardet # biblioteca para identificar a codificacao do arquivo

"""## **1. Extração dos Dados**

Os dados fornecidos no portal da CEAPS, são divididos por ano. Sendo assim para a estruturação da nossa análise, estarei realizando um merge nas bases de dados e separando-as por ano, criando apenas um dataset.
"""

# Criando uma lista de caminhos para cada dataset
caminhos = []

for n in range(2008,2023):
  caminho = f'/content/drive/MyDrive/Colab Notebooks/analise_dados_ceaps/dados_ceaps/despesa_ceaps_{n}.csv'
  caminho = str(caminho)
  caminhos.append(caminho)

caminhos

"""Antes de iniciarmos a junção dos dados, vamos analisar como eles estão estruturados, para entendermos os possíveis tratamentos necessários."""

def detectar_codificacao_arquivo(caminho_arquivo):
  with open(caminho_arquivo, 'rb') as arquivo:
    resultado = chardet.detect(arquivo.read(10000))
    return resultado['encoding']

caminho_2008 = caminhos[0]
codificacao = detectar_codificacao_arquivo(caminho_2008)

# Lendo uma prévia do arquivo, antes de transformalo em um dataset
with open(caminho_2008, 'r', encoding=codificacao) as arq:
  linhas = arq.readlines()

linhas[0:5]

"""Lendo as primeiras linhas do arquivo, podemos verificar algumas coisas:

* A primeira linha contém a data de atualização dos dados, o que pode dificultar a leitura do arquivo por parte do pandas, uma vez que a estrutura da planilha está em um formato não usual.

* O separador utilizado é o ";", o que também pode prejudicar a leitura.

Sendo assim, vamos cuidar para que as alterações necessárias sejam feitas para conseguirmos o nosso objeto dataframe:
"""

dados_2008 = pd.read_csv(caminho_2008, encoding=codificacao, sep=";", skiprows=1)

dados_2008.head()

"""Com a leitura realizada adequadamente, vamos realizar o mesmo processo para todos os arquivos, antes de juntá-los."""

dataframes = []

def aloca_dataframes(lista_caminhos, codificacao):
  for caminho in lista_caminhos:
    dado = pd.read_csv(caminho, encoding=codificacao, sep=';',skiprows=1)
    dataframes.append(dado)
  return dataframes

dataframes = aloca_dataframes(caminhos, codificacao)

# Verificando se todos os dataframes foram alocados
for i in range(0,15):
 print(dataframes[i].ANO[0])

"""Após criarmos nossa lista de dataframes, o próximo passo será juntar todos estes dados em um único dataframe, separado por ano."""

dados_ceaps = pd.concat(dataframes, ignore_index=True)

dados_ceaps.head()

dados_ceaps.shape

dados_ceaps['ANO'].unique()

"""Agora que os dados foram juntados em uma única base, podemos prosseguir com os tratamentos futuros da base e com a análise."""
